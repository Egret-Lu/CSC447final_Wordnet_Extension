{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568f1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EmojiSentiWordnet.sentiwordnet import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.metrics import accuracy_score\n",
    "wnl = WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1cb77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "esw=mojiSentiWordnet()\n",
    "cry_synset=esw.synset(lemma='ðŸ˜­')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b924c96",
   "metadata": {},
   "source": [
    "Here we have 127 emoji icons in the wordnet, can be more if we have more rich dataset. For each emoji character we can get a positive score, a negative score and occurance probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c74acd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The positive score of ðŸ˜­:0.019443217\n",
      "The negative score of ðŸ˜­:0.980556783\n",
      "The occurance rate of ðŸ˜­:0.0511670928\n",
      "The unicode id of ðŸ˜­:1F62D\n",
      "The description of ðŸ˜­::loudly_crying_face:\n"
     ]
    }
   ],
   "source": [
    "print(f'The positive score of ðŸ˜­:{cry_synset.get_pscore()}')\n",
    "print(f'The negative score of ðŸ˜­:{cry_synset.get_nscore()}')\n",
    "print(f'The occurance rate of ðŸ˜­:{cry_synset.get_occ()}')\n",
    "print(f'The unicode id of ðŸ˜­:{cry_synset.get_id()}')\n",
    "print(f'The description of ðŸ˜­:{cry_synset.get_des()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada0a48",
   "metadata": {},
   "source": [
    "Here we test the tool on a sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da5c8b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmfaoo  ðŸ˜­  ðŸ˜­  ðŸ˜­  ðŸ˜­  ðŸ˜­</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i hate this feeling  ðŸ˜¢</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>can't believe i just went out in this cold to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need a new trap house, so if you really fuck...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; so very sorry for your loss.  ðŸ’”</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  labels\n",
       "0                             lmfaoo  ðŸ˜­  ðŸ˜­  ðŸ˜­  ðŸ˜­  ðŸ˜­        0\n",
       "1                            i hate this feeling  ðŸ˜¢        0\n",
       "2  can't believe i just went out in this cold to ...       0\n",
       "3  i need a new trap house, so if you really fuck...       0\n",
       "4            <user> so very sorry for your loss.  ðŸ’”        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df=pd.read_csv('tweet_senti.txt',sep=',')\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9ffabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    text_token=word_tokenize(text)\n",
    "    text_cleaned=[]\n",
    "    for t in text_token :\n",
    "        t=t.lower()\n",
    "        if len(t)>1 and (t not in stopwords):\n",
    "            text_cleaned.append(wnl.lemmatize(t))\n",
    "    return ' '.join(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5ba33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need new trap house really fuck'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='i need a new trap house, so if you really fuck ðŸ˜¢'\n",
    "clean_tweet(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a7a67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df[\"cleaned_tweets\"]=tweet_df[\"tweets\"].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b18628",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), lowercase=True, use_idf=True,max_features=200000)\n",
    "tweet_tfidf=tfidf.fit_transform(tweet_df[\"cleaned_tweets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f53dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=tweet_df[\"tweets\"]\n",
    "p_scores=[]\n",
    "n_scores=[]\n",
    "for tweet in tweets:\n",
    "    p_score=0\n",
    "    n_score=0\n",
    "    tweet_emoji=set(adv.extract_emoji(tweet)['emoji_flat'])\n",
    "    for emoji_icon in tweet_emoji:\n",
    "        if emoji_icon in esw.dict.keys():\n",
    "            emoji_synset=esw.synset(lemma=emoji_icon)\n",
    "            p_score+=emoji_synset.get_pscore()\n",
    "            n_score+=emoji_synset.get_nscore()\n",
    "    p_scores.append(p_score) \n",
    "    n_scores.append(n_score) \n",
    "p_scores=np.array([p_scores]).T\n",
    "n_scores=np.array([n_scores]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c6b1618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13200, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "019659a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1=tweet_tfidf\n",
    "X_2=hstack([tweet_tfidf,coo_matrix(p_scores),coo_matrix(n_scores)])\n",
    "Y=tweet_df[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c91dff06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13200, 65798)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d5bc4",
   "metadata": {},
   "source": [
    "Without Emoji Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5afcb689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dataset: 0.896969696969697\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_1, Y, test_size=0.1)\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "lr.fit(X_train, Y_train)\n",
    "ypred=lr.predict(X_test)\n",
    "print('Accuracy on dataset:', accuracy_score(ypred, Y_test))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40760d3a",
   "metadata": {},
   "source": [
    "With Emoji Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c88e47fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dataset: 0.9598484848484848\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_2, Y, test_size=0.1)\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "lr.fit(X_train, Y_train)\n",
    "ypred=lr.predict(X_test)\n",
    "print('Accuracy on dataset:', accuracy_score(ypred, Y_test))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e0b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from collections import defaultdict\n",
    "import os\n",
    "class synset():\n",
    "    def __init__(self,pos_score=None,neg_score=None,uid=None,occ_rate=None,des=None):\n",
    "        '''\n",
    "        build a sysnet object for each emoji character\n",
    "        '''\n",
    "        self.__ps=pos_score #positive score\n",
    "        self.__ns=neg_score #negative score\n",
    "        self.__occ=occ_rate #occurance rate\n",
    "        self.__uid=uid #id in unicode\n",
    "        self.__des=des #description\n",
    "    def get_pscore(self):\n",
    "        return self.__ps\n",
    "    def get_nscore(self):\n",
    "        return self.__ns\n",
    "    def get_occ(self):\n",
    "        return self.__occ\n",
    "    def get_des(self):\n",
    "        return self.__des\n",
    "    def get_id(self):\n",
    "        return self.__uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77810a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=synset()\n",
    "hasattr(a.__class__, 'get_pscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bf62c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
